{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次实验的数据集与Lab3一致，任务都是识别手写数字0-9。不过在本次实验中，我们需要采用**神经网络**来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ...,\n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loadmat('ex4data1.mat')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们后续需要经常用到样本集，所以我们定义一个别名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['X']\n",
    "y = data['y']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来为y标签进行one-hot编码，将10个类标签转换为长度为10的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youthPaul\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = encoder.fit_transform(y)\n",
    "y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_onehot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "阅读PDF可以得知，我们可以在神经网络的隐藏层中维护25个激活单元。根据我们的数据集维度可以得知，神经网络的输入单元有400个，输出单元有10个\n",
    "\n",
    "<img src=\"image/1.png\" width=\"700\" height=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们来实现**前向传播**和**代价函数**\n",
    "\n",
    "这里需要注意的是，输入层一共有401个激活单元（400个原始输入和1个偏差项），隐藏层一共有26个激活单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forwardPropagation(X, theta1, theta2):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    a1 = np.insert(X, 0, values=np.ones(m), axis=1)\n",
    "    z2 = a1 * theta1.T\n",
    "    a2 = np.insert(sigmoid(z2), 0, values=np.ones(m), axis=1)\n",
    "    z3 = a2 * theta2.T\n",
    "    h = sigmoid(z3)\n",
    "\n",
    "    return a1, z2, a2, z3, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代价函数\n",
    "\n",
    "$J(\\Theta) = -\\frac{1}{m}\\sum\\limits_{i=1}^{m} \\sum\\limits_{k = 1}^{K}{\\Big[{{y}_{k}^{(i)}}\\log \\left( {h_\\theta}\\left( {{x}^{(i)}} \\right) \\right)_{k}+\\left( 1-{{y}_{k}^{(i)}} \\right)\\log \\left( 1-{h_\\theta}\\left( {{x}^{(i)}} \\right) \\right)_{k} \\Big]}$\n",
    "\n",
    "注意此处并没有加入正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(params, input_size, hidden_size, output_size, X, y, lamb):\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "\n",
    "    # reshape the parameter array to parameter matrix for each layer\n",
    "    theta1 = np.matrix(np.reshape(params[: hidden_size * (input_size + 1)], (hidden_size, input_size + 1)))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1) :], (output_size, hidden_size + 1)))\n",
    "\n",
    "    # run the forward propagation\n",
    "    a1, z2, a2, z3, h = forwardPropagation(X, theta1, theta2)\n",
    "\n",
    "    # compute the cost\n",
    "    J = 0\n",
    "    for i in range(m):\n",
    "        first = np.multiply(y[i, :], np.log(h[i, :]))\n",
    "        second = np.multiply(1 - y[i, :], np.log(1 - h[i, :]))\n",
    "        J += np.sum(first + second)\n",
    "    J /= -m\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化各参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 401), (10, 26))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 400\n",
    "hidden_size = 25\n",
    "output_size = 10\n",
    "lamb = 1\n",
    "\n",
    "# random initialize\n",
    "params = (np.random.random(size=hidden_size * (input_size + 1) + output_size * (hidden_size + 1)) - 0.5) * 0.25\n",
    "\n",
    "X = np.matrix(X)\n",
    "y = np.matrix(y)\n",
    "\n",
    "# reshape the parameter array to parameter matrix for each layer\n",
    "theta1 = np.matrix(np.reshape(params[: hidden_size * (input_size + 1)], (hidden_size, input_size + 1)))\n",
    "theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1) :], (output_size, hidden_size + 1)))\n",
    "\n",
    "theta1.shape, theta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 401), (5000, 25), (5000, 26), (5000, 10), (5000, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1, z2, a2, z3, h = forwardPropagation(X, theta1, theta2)\n",
    "a1.shape, z2.shape, a2.shape, z3.shape, h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算一下误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.093594810735645"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(params, input_size, hidden_size, output_size, X, y_onehot, lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化代价函数\n",
    "\n",
    "<img src=\"image/2.png\" width=\"900\" height=\"215\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(params, input_size, hidden_size, output_size, X, y, lamb):\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "\n",
    "    # reshape the parameter array to parameter matrix for each layer\n",
    "    theta1 = np.matrix(np.reshape(params[: hidden_size * (input_size + 1)], (hidden_size, input_size + 1)))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1) :], (output_size, hidden_size + 1)))\n",
    "\n",
    "    # run the forward propagation\n",
    "    a1, z2, a2, z3, h = forwardPropagation(X, theta1, theta2)\n",
    "\n",
    "    # compute the cost\n",
    "    J = 0\n",
    "    for i in range(m):\n",
    "        first = np.multiply(y[i, :], np.log(h[i, :]))\n",
    "        second = np.multiply(1 - y[i, :], np.log(1 - h[i, :]))\n",
    "        J += np.sum(first + second)\n",
    "    J /= -m\n",
    "\n",
    "    # regularize\n",
    "    J += lamb / (2 * m) * (np.sum(np.power(theta1[:, 1:], 2)) + np.sum(np.power(theta2[:, 1:], 2)))\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.098852378268576"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(params, input_size, hidden_size, output_size, X, y_onehot, lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们实现**反向传播**，我们知道，输出层的误差 $\\delta^{(3)} = h - y$\n",
    "\n",
    "而隐藏层的误差 $\\delta^{(2)} = (\\Theta^{(2)})^T \\delta^{(3)} * g^{\\prime}\\big( z^{(2)} \\big)$\n",
    "\n",
    "由于sigmoid的特殊性，$g^{\\prime}(z) = g(z) * (1 - g(z))$，所以我们可以将其导数编写为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    return np.multiply(sigmoid(z), (1 - sigmoid(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们利用**反向传播**来计算梯度\n",
    "\n",
    "<img src=\"image/3.png\" width=\"700\" height=\"330\">\n",
    "\n",
    "可以将计算向量化为：$\\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l + 1)} (a^{(l)})^T$\n",
    "\n",
    "最后我们有：\n",
    "\n",
    "$D_{ij}^{(l)} := \\frac{1}{m}\\Delta_{ij}^{(l)} + \\frac{\\lambda}{m}\\Theta_{ij}^{(l)} \\quad if\\; j \\neq 0$\n",
    "\n",
    "$D_{ij}^{(l)} := \\frac{1}{m}\\Delta_{ij}^{(l)} \\hspace{40pt} if\\; j = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPropagation(params, input_size, hidden_size, output_size, X, y, lamb):\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "\n",
    "    # reshape the parameter array to parameter matrix for each layer\n",
    "    theta1 = np.matrix(np.reshape(params[: hidden_size * (input_size + 1)], (hidden_size, input_size + 1)))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1) :], (output_size, hidden_size + 1)))\n",
    "\n",
    "    # run the forward propagation\n",
    "    a1, z2, a2, z3, h = forwardPropagation(X, theta1, theta2)\n",
    "\n",
    "    J = 0\n",
    "    delta1 = np.zeros(theta1.shape)\n",
    "    delta2 = np.zeros(theta2.shape)\n",
    "\n",
    "    # compute the cost\n",
    "    for i in range(m):\n",
    "        first = np.multiply(y[i, :], np.log(h[i, :]))\n",
    "        second = np.multiply(1 - y[i, :], np.log(1 - h[i, :]))\n",
    "        J += np.sum(first + second)\n",
    "    J /= -m\n",
    "\n",
    "    # regularize\n",
    "    J += lamb / (2 * m) * (np.sum(np.power(theta1[:, 1:], 2)) + np.sum(np.power(theta2[:, 1:], 2)))\n",
    "\n",
    "    # backprop\n",
    "    for i in range(m):\n",
    "        yt = y[i, :] # (1, 10)\n",
    "        ht = h[i, :] # (1, 10)\n",
    "        z2t = z2[i, :] # (1, 25)\n",
    "        a2t = a2[i, :] # (1, 26)\n",
    "        a1t = a1[i, :] # (1, 401)\n",
    "\n",
    "        d3 = ht - yt # (1, 10)\n",
    "        d2 = np.multiply((theta2.T * d3.T).T[:, 1:], sigmoid_gradient(z2t)) # (1, 25)\n",
    "\n",
    "        delta2 += d3.T * a2t\n",
    "        delta1 += d2.T * a1t\n",
    "\n",
    "    delta1 /= m\n",
    "    delta2 /= m\n",
    "\n",
    "    # regularize\n",
    "    delta1[:, 1:] = delta1[:, 1:] + (theta1[:, 1:] * lamb) / m\n",
    "    delta2[:, 1:] = delta2[:, 1:] + (theta2[:, 1:] * lamb) / m\n",
    "\n",
    "    # unravel the gradient matrices to a single array\n",
    "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试运行，看看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.098852378268576, (10285,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J, grad = backwardPropagation(params, input_size, hidden_size, output_size, X, y_onehot, lamb)\n",
    "J, grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们进行梯度下降。由于目标函数可能不收敛，我们显式限制迭代次数为250次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " message: Max. number of function evaluations reached\n",
       " success: False\n",
       "  status: 3\n",
       "     fun: 0.3356948692482169\n",
       "       x: [-1.343e+00  1.338e-03 ...  8.923e-01 -9.735e-01]\n",
       "     nit: 20\n",
       "     jac: [-3.971e-04  2.677e-07 ... -3.841e-05 -6.471e-05]\n",
       "    nfev: 250"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# minimize the objective function\n",
    "fmin = minimize(fun=backwardPropagation, x0=params, args=(input_size, hidden_size, output_size,\n",
    "                 X, y_onehot, lamb), method='TNC', jac=True, options={'maxfun': 250})\n",
    "fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到最终的损失值已经降到0.5以下，已经最够优秀\n",
    "\n",
    "接下来我们利用训练好的神经网络进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10],\n",
       "       [10],\n",
       "       [10],\n",
       "       ...,\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 9]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.matrix(X)\n",
    "theta1 = np.matrix(np.reshape(fmin.x[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "theta2 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size + 1):], (output_size, (hidden_size + 1))))\n",
    "\n",
    "a1, z2, a2, z3, h = forwardPropagation(X, theta1, theta2)\n",
    "y_pred = np.array(np.argmax(h, axis=1) + 1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们计算准确度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 99.26%\n"
     ]
    }
   ],
   "source": [
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]\n",
    "accuracy = sum(correct) / len(correct)\n",
    "print('accuracy = {0}%'.format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
